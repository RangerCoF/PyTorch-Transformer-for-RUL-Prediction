{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(1)\n",
    "from model import *\n",
    "from loading_data_tensor import *\n",
    "from testing import *\n",
    "from visualize import *\n",
    "import os\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "# import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--dataset', type=str, default='FD001', help='which dataset to run')\n",
    "# opt = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 4 # Number of training epochs\n",
    "d_model = 128  # dimension in encoder\n",
    "heads = 4  # number of heads in multi-head attention\n",
    "N = 2  # number of encoder layers\n",
    "m = 14  # number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "group, y_test, group_test = loading_FD001()\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 17)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group.get_group(1).to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(287, 17)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group.get_group(2).to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = group.get_group(1).to_numpy()\n",
    "total_loss = 0\n",
    "model = Transformer(m, d_model, N, heads, dropout)\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(x.shape[0] - 1):\n",
    "    if t == 0:  # skip the first and last for convolution without padding\n",
    "        continue\n",
    "    else:\n",
    "        X = x[t - 1:t + 2, 2:-1]  # fetch the 3 * 14 feature as input\n",
    "\n",
    "    y = x[t, -1:]  # fetch the corresponding target rul as label\n",
    "\n",
    "    X_train_tensors = Variable(torch.Tensor(X))\n",
    "    y_train_tensors = Variable(torch.Tensor(y))\n",
    "    \n",
    "    X_train_tensors_final = X_train_tensors.reshape(\n",
    "        (1, 1, X_train_tensors.shape[0], X_train_tensors.shape[1]))\n",
    "\n",
    "\n",
    "    # forward pass\n",
    "    outputs = model.forward(X_train_tensors_final, t)\n",
    "\n",
    "\n",
    "    # obtain the loss function\n",
    "    loss = criterion(outputs, y_train_tensors)\n",
    "\n",
    "    # summarize the loss\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    loss = loss / (x.shape[0] - 2)  # normalize the loss\n",
    "    loss.backward()  # backward pass\n",
    "\n",
    "    # only update after finishing one unit\n",
    "    if t == x.shape[0] - 2:  # Wait for several backward steps\n",
    "        optim.step()  # Now we can do an optimizer step\n",
    "        optim.zero_grad()  # Reset gradients tensors\n",
    "\n",
    "# i += 1\n",
    "# epoch_loss += total_loss / x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):  # iteration of epoch\n",
    "    i = 1\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # training step\n",
    "    model.train()\n",
    "\n",
    "    while i <= 100:  # iteration of unit\n",
    "\n",
    "        # fetch the data of unit i\n",
    "        x = group.get_group(i).to_numpy()\n",
    "        total_loss = 0\n",
    "        optim.zero_grad()\n",
    "\n",
    "        for t in range(x.shape[0] - 1):\n",
    "            if t == 0:  # skip the first and last for convolution without padding\n",
    "                continue\n",
    "            else:\n",
    "                X = x[t - 1:t + 2, 2:-1]  # fetch the 3 * 14 feature as input\n",
    "\n",
    "            y = x[t, -1:]  # fetch the corresponding target rul as label\n",
    "\n",
    "            X_train_tensors = Variable(torch.Tensor(X))\n",
    "            y_train_tensors = Variable(torch.Tensor(y))\n",
    "            \n",
    "            X_train_tensors_final = X_train_tensors.reshape(\n",
    "                (1, 1, X_train_tensors.shape[0], X_train_tensors.shape[1]))\n",
    "\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model.forward(X_train_tensors_final, t)\n",
    "\n",
    "\n",
    "            # obtain the loss function\n",
    "            loss = criterion(outputs, y_train_tensors)\n",
    "\n",
    "            # summarize the loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss = loss / (x.shape[0] - 2)  # normalize the loss\n",
    "            loss.backward()  # backward pass\n",
    "\n",
    "            # only update after finishing one unit\n",
    "            if t == x.shape[0] - 2:  # Wait for several backward steps\n",
    "                optim.step()  # Now we can do an optimizer step\n",
    "                optim.zero_grad()  # Reset gradients tensors\n",
    "\n",
    "        i += 1\n",
    "        epoch_loss += total_loss / x.shape[0]\n",
    "\n",
    "    # evaluate model\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        rmse, result = testing(group_test, y_test, model)\n",
    "\n",
    "    print(\"Epoch: %d, training loss: %1.5f, testing rmse: %1.5f\" % (epoch, epoch_loss / 100, rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(result, rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
